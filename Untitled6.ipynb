{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30ad59db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_31592/3288536300.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.mode_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "from sklearn.decomposition import PCA\n",
    "import tensorflow_hub as hub\n",
    "from pycaret.classification import *\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "#from googletrans import Translator\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParama['font.serif'] = 'Ubuntu'\n",
    "plt.rcParama['font.monospace'] = 'Ubuntu Mono'\n",
    "plt.rcParama['font.size'] = 14\n",
    "plt.rcParama['axes.labelsize'] = 12\n",
    "plt.rcParama['axes.labelweight'] = 'bold'\n",
    "plt.rcParama['axes.titlesize'] = 12\n",
    "plt.rcParama['xtick.labelsize'] = 12\n",
    "plt.rcParama['ytick.labelsize'] = 12\n",
    "plt.rcParama['legend.fontsize'] = 12\n",
    "plt.rcParama['figure.titlesize'] = 12\n",
    "plt.rcParama['image.cmap'] = 'jet'\n",
    "plt.rcParama['image.interpolation'] = 'none'\n",
    "plt.rcParama['figure.figsize'] = (10, 10)\n",
    "plt.rcParama['axes.grid'] = False\n",
    "plt.rcParama['lines.linewidth'] = 2\n",
    "plt.rcParama['lines.markersize'] = 8\n",
    "colors = ['xkcd:pale range', 'xkcd:sea blue', 'xkcd:pale red', 'xkcd:sage green', 'xkcd:terra cotta', 'xkcd:dull purple', 'xkcd:teal', 'xkcd: goldenrod', 'xkcd\"cadet blue', 'xkcd:scarlet']\n",
    "bbox_props = dict(boxstyle=\"round,pad=0.3\", fc=colors[0], alpha=.5)\n",
    "import pandas as pd\n",
    "import pycaret"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5a23ad61",
   "metadata": {},
   "source": [
    "true_data = pd.read_csv('True.csv')\n",
    "fake_data = pd.read_csv(\"fake.csv\")\n",
    "true_data.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9929e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_data['Target']=['True']*len(true_data)\n",
    "fake_data['Target']=['Fake']*len(fake_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ea45b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=true_data.append(fake_data).sample(frac=1).reset(frac=1).reset_index().drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4997b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pie(label_size,explode=[0.1,0.1],colors=['firebrick','navy'],stratangle=90,shadow=True,labels=['Fake','True'],autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255afedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label']=pd.get_dummies(data.Target)['Fake']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd252b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text, temp_text, train_labels, temp_labels = train_test_split(data['title', data['label'], random_state=2018, test_size=0.3,stratify=data['Target']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac7e535",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
    "                                                                random_state=2018, \n",
    "                                                                test_size=0.5, \n",
    "                                                                stratify=temp_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4daac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc78cce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = [len(i.split()) for in train _text]\n",
    "\n",
    "pd.Series(seq_len).hist(bins = 40,color='firebrick')\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Number of texts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e909a7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGHT = 15\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text.tolist(),\n",
    "    max_length = MAX_LENGHT,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the validation set\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    val_text.tolist(),\n",
    "    max_length = MAX_LENGHT,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the test set\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text.tolist(),\n",
    "    max_length = MAX_LENGHT,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0dac12",
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert lists to tensors\n",
    "\n",
    "train_serq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "val_y = torch.tensor(test_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ce301d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLOader, RandomSampler, SequentialSampler\n",
    "\n",
    "#define a batch size\n",
    "batch_size = 32\n",
    "\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "#wrap tensors\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "#sampler for sampling the data during training\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "# dataloader for validation set\n",
    "val_datloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b81e1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f8e137",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Arch(nn.Module):\n",
    "    \n",
    "    def __init__(self, bert):\n",
    "        \n",
    "        super(BERT_Arch, self).__init__()\n",
    "        \n",
    "        self.bert = bert\n",
    "        \n",
    "        #dropout layer\n",
    "        self.dropouty = nn.Dropout(0.1)\n",
    "        \n",
    "        #relu activation function\n",
    "        self.relu = nn.ReLu()\n",
    "        \n",
    "        #dense layer 1\n",
    "        self.fc1 = nn.Linear(768,512)\n",
    "        \n",
    "        #dense layer 2 (Output layer)\n",
    "        self.fc2 = nn.Linear(512,2)\n",
    "        \n",
    "        #softmax activation function\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "        \n",
    "        #pass the inputs to the model\n",
    "        cls_hs = self.bert(sent_id. attention_mask=mask)['pooler_output']\n",
    "        x = self.fc1(cls_hs)\n",
    "        \n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # output layer\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        #apply softmax activation\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9ac5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = Adam(model.parameters(), lr = 1e-5)      #learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882fbeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "#compute the class weights\n",
    "class_weights = compute_class_weight('balanced', np.unique(train_labels), train_labels)\n",
    "\n",
    "print(\"Class Weights:\",class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5369e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.tensor(class_weights,dtype=torch.float)\n",
    "\n",
    "#define the loss function\n",
    "cross_entropy = nn.NNLLoss(weight=weights)\n",
    "\n",
    "# number of training epochs\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b375bd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    \n",
    "    # empty list to save model predictions\n",
    "    total_preds=[]\n",
    "    \n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(train_dataloader):\n",
    "        \n",
    "        #progress update after every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print(' Batch {:>5,} of {:>5,}.'.format(step, len(train_dataloader)))\n",
    "         \n",
    "            #push the batch to gpu\n",
    "            batch = [r for r in batch]\n",
    "            sent_id, mask, labels = batch\n",
    "            #print(type(labels),type(mask),type(sent_id))\n",
    "            #print(sent_id)\n",
    "            #clear previously calculated gradients\n",
    "            model.zero_grad()\n",
    "            #get model predictions for the current batch\n",
    "            preds = model(sent_id, mask)\n",
    "            \n",
    "            # compute the loss between actual and predicted values\n",
    "            loss = cross_entropy(preds, labels)\n",
    "            \n",
    "            # add on to the total loss\n",
    "            total_loss = total_loss + loss.item()\n",
    "            \n",
    "            # backward pass to calculate the ingredients\n",
    "            loss.backward()\n",
    "            \n",
    "            # clip the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            #update parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            # model predictions are stored on GPU. So, push it to CPU\n",
    "            preds=preds.detach().cpu().numpy()\n",
    "            \n",
    "            # append the model predictions\n",
    "            total_preds.append(preds)\n",
    "            \n",
    "        # compute the training loss of the epoch\n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "        \n",
    "        #predictions are in the form of(no. of batches, size of batch, no. of classes).\n",
    "        #reshape the predictions in form of (number of samples, no. of classes).\n",
    "        total_preds = np.concatenate(total_preds, axis=0)\n",
    "        \n",
    "        #returns the loss and predictions\n",
    "        return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb27158",
   "metadata": {},
   "source": [
    "def evaluate():\n",
    "    \n",
    "    print(\"\\nEvaluating...\")\n",
    "    \n",
    "    # deactivates dropout layers\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    \n",
    "    # empty list to save the model predictions\n",
    "    total_preds = []\n",
    "    \n",
    "    #iterate over batches\n",
    "    for step,batch in enumerate(val_dataloader):\n",
    "        \n",
    "        #progress update every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            \n",
    "            #Calculate elapsed time in minutes.\n",
    "            #elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            #report progress\n",
    "            print(' Batch{:>5,} of {:>5,}.'.format(step, len(val_dataloader)))\n",
    "            \n",
    "        # push the batch to gpu\n",
    "        batch = [t for t in batch]\n",
    "        \n",
    "        sent_id, mask, labels = batch\n",
    "        \n",
    "        # deactivate autograd\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # model predictions\n",
    "            preds = model(sent_id, mask)\n",
    "            \n",
    "            # compute the validation loss between actual and predicted values\n",
    "            loss = cross_entropy(preds,labels)\n",
    "            \n",
    "            total_loss = total_loss + loss.item()\n",
    "            \n",
    "            preds = preds.detach().cpu().numpy()\n",
    "            \n",
    "            total_preds.append(preds)\n",
    "            \n",
    "        # compute the validation loss of the epoch\n",
    "        avg_loss = total_loss / len(val_dataloader)\n",
    "        \n",
    "        #reshape the predictions in form of (number of samples, no. of classes)\n",
    "        total_preds = np.concatenate(total_preds, axis=0)\n",
    "        \n",
    "        return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7621775d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_valid_loss = float('inf')\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "#for each epoch\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    #train model\n",
    "    train_loss, _ = train()\n",
    "    \n",
    "    #evaluate model\n",
    "    valid_loss, _ = evaluate()\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "        \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'\\nTraining Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8aa16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load weights of best model\n",
    "path = 'saved_weights.pt'\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929a017e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    preds + model(test_seq, test_mask)\n",
    "    preds = preds.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37ffb0d",
   "metadata": {},
   "source": [
    "preds = np.argmax(preds, axis = 1)\n",
    "print(classification_report(test_y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a9fa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(preds, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7338e150",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b59325",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_matrix = embed(data.title.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83771a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data.loc[0:int(len(data)*0.8)]\n",
    "test_data = data.loc[int(len(data)*0.8):len(data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4ae7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "pca_data = pca.fit(data_matrix[0:len(train_data)])\n",
    "pca_train = pca.transform(data_matrix[0:len(train_data)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b50e779",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_3_data = pd.DataFrame({'First Component':pca_train[:,0],'Second Component':pca_train[:,1],'Third Component':pca_train[:,2],'Target': train_data.Target})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4290640",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(1,3,1)\n",
    "sns.scatterplot(x='First Component', y = 'Second Component', hue='Target',data=pca_3_3_data,s=2)\n",
    "plt.grid(True)\n",
    "plt.subplot(1,3,2)\n",
    "sns.scatterplot(x='First Component', y = 'Third Component',hue='Target',data=pca_3_data,s=2)\n",
    "plt.grid(True)\n",
    "plt.subplot(1,3,3)\n",
    "sns.scatterplot(x='Second Component', y = 'Third Component',hue='Target',data=pca_3_data,s=2)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3fb0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pca_3_data['subject']=train_data.subjectenc.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ec221f",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup(data = pca_3_data, target='Target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bdf191",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_moedl = compare_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44df94fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_true = le.fit_transform(test_data.Target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6e65eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_pred,y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1997a09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(best_model,pca_test,y_true,cmap='plasma')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
